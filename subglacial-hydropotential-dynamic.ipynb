{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b9f6b66c-93e3-4a17-97b5-14940183fb01",
   "metadata": {},
   "source": [
    "# TODO\n",
    "* take pixels with no ice thickness and just make the hydropotential the graviational potential"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efebafdf-052e-494a-a484-df11e624d64c",
   "metadata": {},
   "source": [
    "Notebook uses BedMachine Antarctica ice-surface and bed topographies to calculate and export subglacial hydropotential using the Shreve (1972) hydropotential equation.\n",
    "\n",
    "Written 2025/07/20 by W. Sauthoff (wsauthoff.github.io) and M. R. Siegfried (mrsiegfried.github.io)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2882be40-6fe2-4c92-8aad-a3ec56869d3c",
   "metadata": {},
   "source": [
    "# Setup computing environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cab3523-6c42-4eaf-aaf8-87982e1f7fdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import packages\n",
    "import earthaccess\n",
    "import fsspec\n",
    "import geopandas as gpd\n",
    "import glob\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "import numpy as np\n",
    "import os\n",
    "from pyproj import CRS\n",
    "import shutil\n",
    "import xarray as xr\n",
    "import zarr\n",
    "import zipfile\n",
    "\n",
    "os.makedirs('output', exist_ok = True)\n",
    "\n",
    "%matplotlib widget\n",
    "\n",
    "# Define data and script directories dependent on home environment\n",
    "if os.getenv('HOME') == '/home/jovyan':\n",
    "    DATA_DIR = '/home/jovyan/data'\n",
    "    OUTPUT_DIR = '/output'\n",
    "# DATA_DIR = '~/data'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74d527cd-82b5-4733-9c03-e7be44fb8978",
   "metadata": {},
   "source": [
    "# Define functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f920fc8-7761-470d-8349-a4cade8b8da3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_quadrants_from_files(input_data, ds_prefixes, x_dim='x', y_dim='y', keep_vars=None):\n",
    "    \"\"\"\n",
    "    Combine one or more quadrant NetCDF datasets into a single dataset using xarray.\n",
    "    Automatically detects which quadrants (A1–A4) are available and concatenates accordingly.\n",
    "    If only one quadrant is provided, it is returned as-is (no trimming or merging).\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    input_data : str\n",
    "        Directory path containing the quadrant NetCDF files.\n",
    "    ds_prefixes : list of str\n",
    "        List of dataset prefixes corresponding to available quadrants.\n",
    "        Example: ['CryoSat2_SARIn_delta_h_A1', 'CryoSat2_SARIn_delta_h_A2']\n",
    "    x_dim, y_dim : str\n",
    "        Names of the x and y dimensions.\n",
    "    keep_vars : list, optional\n",
    "        List of variables to keep (coordinates always preserved).\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    xarray.Dataset\n",
    "        Combined dataset across available quadrants, with overlaps removed if applicable.\n",
    "    \"\"\"\n",
    "\n",
    "    def drop_unwanted_variables(dataset, keep_vars):\n",
    "        \"\"\"Drop variables not listed in keep_vars, preserving coordinates.\"\"\"\n",
    "        if keep_vars is None:\n",
    "            return dataset\n",
    "        coord_vars = set(dataset.coords.keys())\n",
    "        vars_to_preserve = set(keep_vars) | coord_vars\n",
    "        vars_to_drop = [v for v in dataset.variables if v not in vars_to_preserve]\n",
    "        return dataset.drop_vars(vars_to_drop, errors='ignore')\n",
    "\n",
    "    quadrant_order = {'A1': 0, 'A2': 1, 'A3': 2, 'A4': 3}\n",
    "    ordered_datasets = [None, None, None, None]\n",
    "\n",
    "    # Load each available dataset\n",
    "    for prefix in ds_prefixes:\n",
    "        found_suffix = next((suffix for suffix in quadrant_order if prefix.endswith(suffix)), None)\n",
    "        if found_suffix is None:\n",
    "            raise ValueError(f\"Prefix '{prefix}' does not end with a valid quadrant (A1–A4).\")\n",
    "\n",
    "        if not isinstance(input_data, str):\n",
    "            raise TypeError(\"input_data must be a directory path (str).\")\n",
    "\n",
    "        pattern = os.path.join(input_data, f\"{prefix}.nc\")\n",
    "        files = glob.glob(pattern)\n",
    "        if not files:\n",
    "            raise ValueError(f\"No file found for pattern: {pattern}\")\n",
    "\n",
    "        ds = xr.open_dataset(files[0])\n",
    "        ds = drop_unwanted_variables(ds, keep_vars)\n",
    "        ordered_datasets[quadrant_order[found_suffix]] = ds\n",
    "\n",
    "    # Filter out missing quadrants\n",
    "    datasets = [d for d in ordered_datasets if d is not None]\n",
    "    available_quadrants = [q for q, idx in quadrant_order.items() if ordered_datasets[idx] is not None]\n",
    "    print(f\"→ Found {len(datasets)} quadrant(s): {', '.join(available_quadrants)}\")\n",
    "\n",
    "    # Case 1: Only one quadrant — return as-is (no edge trimming)\n",
    "    if len(datasets) == 1:\n",
    "        print(\"→ Only one quadrant found — skipping trimming and concatenation.\")\n",
    "        return datasets[0]\n",
    "\n",
    "    # Case 2: Multiple quadrants — apply trimming\n",
    "    processed = []\n",
    "    for q, ds in zip(available_quadrants, datasets):\n",
    "        if q == 'A1':\n",
    "            processed.append(ds)\n",
    "        elif q == 'A2':\n",
    "            processed.append(ds.drop_sel({x_dim: 0}, errors='ignore'))\n",
    "        elif q == 'A3':\n",
    "            processed.append(ds.drop_sel({y_dim: 0}, errors='ignore'))\n",
    "        elif q == 'A4':\n",
    "            processed.append(ds.drop_sel({x_dim: 0, y_dim: 0}, errors='ignore'))\n",
    "\n",
    "    # Concatenate depending on which quadrants exist\n",
    "    if set(available_quadrants) == {'A1', 'A2'}:\n",
    "        combined = xr.concat([processed[1].isel({x_dim: slice(0, -1)}), processed[0]], dim=x_dim)\n",
    "    elif set(available_quadrants) == {'A3', 'A4'}:\n",
    "        combined = xr.concat([processed[0].isel({x_dim: slice(0, -1)}), processed[1]], dim=x_dim)\n",
    "    elif set(available_quadrants) == {'A1', 'A3'}:\n",
    "        combined = xr.concat([processed[1].isel({y_dim: slice(0, -1)}), processed[0]], dim=y_dim)\n",
    "    elif set(available_quadrants) == {'A2', 'A4'}:\n",
    "        combined = xr.concat([processed[1].isel({y_dim: slice(0, -1)}), processed[0]], dim=y_dim)\n",
    "    elif len(processed) == 4:\n",
    "        A12 = xr.concat([processed[1].isel({x_dim: slice(0, -1)}), processed[0]], dim=x_dim)\n",
    "        A34 = xr.concat([processed[2].isel({x_dim: slice(0, -1)}), processed[3]], dim=x_dim)\n",
    "        combined = xr.concat([A34.isel({y_dim: slice(0, -1)}), A12], dim=y_dim)\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported combination of quadrants: {available_quadrants}\")\n",
    "\n",
    "    print(f\"→ Combined dataset shape: {combined[x_dim].size} × {combined[y_dim].size}\")\n",
    "    return combined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d39c9f58-d05e-43df-a791-f712b3662445",
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_quadrants_from_files_S3(\n",
    "    datasets_files,\n",
    "    ds_prefixes,\n",
    "    group=None,\n",
    "    keep_vars=None,\n",
    "    x_dim='x',\n",
    "    y_dim='y',\n",
    "    retries=3\n",
    "):\n",
    "    \"\"\"\n",
    "    Open, filter, and combine one or more ATL15 quadrant datasets (A1–A4).\n",
    "    Works with S3 file-like objects. If only one quadrant is provided,\n",
    "    returns it unmodified (no trimming or concatenation).\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    datasets_files : list\n",
    "        List of file-like objects (e.g., S3 paths or fsspec file objects).\n",
    "    ds_prefixes : list of str\n",
    "        Dataset prefixes such as ['ATL15_A1', 'ATL15_A2', ...].\n",
    "    group : str\n",
    "        NetCDF group to open (default None opens root level).\n",
    "    keep_vars : list, optional\n",
    "        Variables to keep; coordinates always preserved.\n",
    "    x_dim, y_dim : str\n",
    "        Names of horizontal dimensions.\n",
    "    retries : int\n",
    "        Number of retry attempts when opening datasets.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    xarray.Dataset\n",
    "        Combined dataset if multiple quadrants given; otherwise, a single dataset.\n",
    "    \"\"\"\n",
    "\n",
    "    def drop_unwanted_variables(dataset):\n",
    "        if keep_vars is None:\n",
    "            return dataset\n",
    "        coord_vars = set(dataset.coords.keys())\n",
    "        vars_to_preserve = set(keep_vars) | coord_vars\n",
    "        vars_to_drop = [v for v in dataset.variables if v not in vars_to_preserve]\n",
    "        return dataset.drop_vars(vars_to_drop, errors='ignore')\n",
    "\n",
    "    def safe_open_and_filter(file, group=group):\n",
    "        for attempt in range(retries):\n",
    "            try:\n",
    "                print(f\"Opening file: {file} (Attempt {attempt+1})\")\n",
    "                ds = xr.open_dataset(file, group=group)\n",
    "                print(f\"Successfully opened file: {file}\")\n",
    "                return drop_unwanted_variables(ds)\n",
    "            except Exception as e:\n",
    "                print(f\"Attempt {attempt+1} failed for {file}: {e}\")\n",
    "                if attempt < retries - 1:\n",
    "                    print(\"Retrying...\")\n",
    "                else:\n",
    "                    raise e\n",
    "\n",
    "    # --- Load datasets matching prefixes ---\n",
    "    quadrant_order = {'A1': 0, 'A2': 1, 'A3': 2, 'A4': 3}\n",
    "    ordered_datasets = [None, None, None, None]\n",
    "\n",
    "    for prefix in ds_prefixes:\n",
    "        found_suffix = next((suffix for suffix in quadrant_order if prefix.endswith(suffix)), None)\n",
    "        if found_suffix is None:\n",
    "            raise ValueError(f\"Prefix '{prefix}' does not end with a valid quadrant (A1–A4).\")\n",
    "\n",
    "        # Match file by substring (works with S3 paths)\n",
    "        matching_files = [f for f in datasets_files if prefix in str(f)]\n",
    "        if not matching_files:\n",
    "            print(f\"No file matched for prefix: {prefix}\")\n",
    "            continue\n",
    "\n",
    "        ds = safe_open_and_filter(matching_files[0], group=group)\n",
    "        ordered_datasets[quadrant_order[found_suffix]] = ds\n",
    "\n",
    "    # --- Filter out missing quadrants ---\n",
    "    datasets = [d for d in ordered_datasets if d is not None]\n",
    "    available_quadrants = [q for q, idx in quadrant_order.items() if ordered_datasets[idx] is not None]\n",
    "    print(f\"→ Found {len(datasets)} quadrant(s): {', '.join(available_quadrants)}\")\n",
    "\n",
    "    # --- Case 1: Single quadrant → return unmodified ---\n",
    "    if len(datasets) == 1:\n",
    "        print(\"→ Only one quadrant found — skipping trimming and concatenation.\")\n",
    "        return datasets[0]\n",
    "\n",
    "    # --- Case 2: Multiple quadrants → apply overlap trimming ---\n",
    "    processed = []\n",
    "    for q, ds in zip(available_quadrants, datasets):\n",
    "        if q == 'A1':\n",
    "            processed.append(ds)\n",
    "        elif q == 'A2':\n",
    "            processed.append(ds.drop_sel({x_dim: 0}, errors='ignore'))\n",
    "        elif q == 'A3':\n",
    "            processed.append(ds.drop_sel({y_dim: 0}, errors='ignore'))\n",
    "        elif q == 'A4':\n",
    "            processed.append(ds.drop_sel({x_dim: 0, y_dim: 0}, errors='ignore'))\n",
    "\n",
    "    # --- Combine according to which quadrants exist ---\n",
    "    if set(available_quadrants) == {'A1', 'A2'}:\n",
    "        combined = xr.concat([processed[1].isel({x_dim: slice(0, -1)}), processed[0]], dim=x_dim)\n",
    "    elif set(available_quadrants) == {'A3', 'A4'}:\n",
    "        combined = xr.concat([processed[0].isel({x_dim: slice(0, -1)}), processed[1]], dim=x_dim)\n",
    "    elif set(available_quadrants) == {'A1', 'A3'}:\n",
    "        combined = xr.concat([processed[1].isel({y_dim: slice(0, -1)}), processed[0]], dim=y_dim)\n",
    "    elif set(available_quadrants) == {'A2', 'A4'}:\n",
    "        combined = xr.concat([processed[1].isel({y_dim: slice(0, -1)}), processed[0]], dim=y_dim)\n",
    "    elif len(processed) == 4:\n",
    "        A12 = xr.concat([processed[1].isel({x_dim: slice(0, -1)}), processed[0]], dim=x_dim)\n",
    "        A34 = xr.concat([processed[2].isel({x_dim: slice(0, -1)}), processed[3]], dim=x_dim)\n",
    "        combined = xr.concat([A34.isel({y_dim: slice(0, -1)}), A12], dim=y_dim)\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported combination of quadrants: {available_quadrants}\")\n",
    "\n",
    "    print(f\"→ Combined dataset shape: {combined[x_dim].size} × {combined[y_dim].size}\")\n",
    "    return combined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e02e99e6-c6b1-4982-82d6-fe80e501fe4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def selective_clip(ds, \n",
    "                   SARIn_expand_date=np.datetime64('2013-10-01T22:30:00.000000000'),\n",
    "                   geom_early=None, \n",
    "                   geom_late=None\n",
    "                  ):\n",
    "\n",
    "    # Dynamic CRS assignment\n",
    "    # Extract the CRS info (EPSG code or WKT) from the grid mapping variable\n",
    "    # stored in your dataset.\n",
    "    try:\n",
    "        # Try to grab the simple EPSG string first\n",
    "        crs_info = ds['spatial_ref'].attrs['spatial_ref']\n",
    "    except KeyError:\n",
    "        # Fallback to WKT if 'spatial_ref' isn't there\n",
    "        crs_info = ds['spatial_ref'].attrs['crs_wkt']\n",
    "        \n",
    "    # Apply the CRS to the current chunk\n",
    "    ds.rio.write_crs(crs_info, inplace=True)\n",
    "    \n",
    "    if ds.time < SARIn_expand_date:\n",
    "        return ds.rio.clip(geom_early, drop=False)\n",
    "    elif SARIn_expand_date <= ds.time:\n",
    "        return ds.rio.clip(geom_late, drop=False)\n",
    "    else:\n",
    "        return ds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea9cdd94-4d3c-486b-89e3-b1280cc1a623",
   "metadata": {},
   "source": [
    "# Access data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c792f396-a660-4d56-a6c3-059f33c0cf53",
   "metadata": {},
   "source": [
    "## Boundaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6063c580-4290-46ad-a0e1-8a2fd96528dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #TODO: download stationary outline gdf's; must rename the output in Sauthoff-2025-GRL\n",
    "# # try to get wildcard operator to work: #-w \"*.geojson\"\n",
    "# !zenodo_get -o ./input/lake_outlines/Sauthoff_2025_GRL/temp 10.5281/zenodo.15758712"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f12cdec5-de97-4fb3-8083-c1ab8c8b1531",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #TODO: download stationary outline gdf's; must rename the output in Sauthoff-2025-GRL\n",
    "\n",
    "# # Import Sauthoff, 2025 (active subglacial lake evolving outlines)\n",
    "# # https://doi.org/10.5281/zenodo.14963551\n",
    "# !zenodo_get -o ./input/lake_outlines/Sauthoff_2025_GRL/evolving_outlines 10.5281/zenodo.15758712 -w \"Sauthoff-2025-GRL/output/lake_outlines/evolving_outlines/*.geojson\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21376749-c561-4b14-880d-c5d45c1c6f30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !zenodo_get -l 10.5281/zenodo.15758712"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a85b9dd-d0e6-460e-8bbc-03a74e65f072",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #TODO: Maybe try adding evolving_outlines to the file names\n",
    "# !zenodo_get -o ./input/lake_outlines/Sauthoff_2025_GRL/evolving_outlines 10.5281/zenodo.15758712 -w \"*evolving_outlines*.geojson\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "294546fc-441e-4ba0-ad8f-b1c3c9fe6267",
   "metadata": {},
   "outputs": [],
   "source": [
    "# might not be working because zenodo data is zipped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "102d7f74-818e-4490-8fdf-3b9cdb165502",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import CryoSat-2 SARIn mode mask\n",
    "# repo: https://github.com/wsauthoff/Sauthoff-2025-GRL (https://doi.org/10.5281/zenodo.15758712)\n",
    "# See 0_preprocess_data.ipynb for data source and pre-processing steps within Sauthoff and others, 202X\n",
    "#TODO: change to access from one Sauthoff_2025_GRL folder\n",
    "# gdf_SARIn_3_1 = gpd.read_file('input/CS2_SARIn_mode_masks/gdf_SARIn_3_1.geojson')\n",
    "# gdf_SARIn_3_6 = gpd.read_file('input/CS2_SARIn_mode_masks/gdf_SARIn_3_6.geojson')\n",
    "\n",
    "\n",
    "gdf_SARIn_3_1 = gpd.read_file('input/Sauthoff_2025_GRL/gdf_SARIn_3_1.geojson')\n",
    "gdf_SARIn_3_6 = gpd.read_file('input/Sauthoff_2025_GRL/gdf_SARIn_3_6.geojson')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e491b6bd-456d-43f9-9dec-3b299ab6ff9e",
   "metadata": {},
   "source": [
    "## BedMachine Antarctica"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b20178a3-5275-4236-8ed8-4f492bec89d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Direct-access stream BedMachine Antarctica bed and surface topography v3 (Morlighem et al., 2020; Morlighem, 2022)\n",
    "# https://nsidc.org/data/NSIDC-0756\n",
    "try:\n",
    "    results = earthaccess.search_data(\n",
    "        doi='10.5067/FPSU0V1MWUB6',\n",
    "        cloud_hosted=True,\n",
    "        # bounding_box=(1, -89, -1, -89)\n",
    "    )\n",
    "    \n",
    "    if not results:\n",
    "        raise ValueError('No datasets found matching the search criteria')\n",
    "        \n",
    "    # Open data granules as s3 files to stream\n",
    "    files = earthaccess.open(results)\n",
    "    \n",
    "    # Check if files list is empty\n",
    "    if not files:\n",
    "        raise FileNotFoundError('No files were opened from the search results')\n",
    "        \n",
    "    # Check if first file exists/is valid\n",
    "    if isinstance(files[0], FileNotFoundError):\n",
    "        raise FileNotFoundError(f'Could not access file: {files[0]}')\n",
    "        \n",
    "    # Print file name to ensure expected dataset\n",
    "    print(f'Attempting to open: {files[0]}')\n",
    "    \n",
    "    # Open file into xarray dataset\n",
    "    bedmachine = xr.open_dataset(files[0], engine='h5netcdf')\n",
    "    print('Dataset successfully loaded')\n",
    "\n",
    "except FileNotFoundError as e:\n",
    "    print(f'File access error: {e}')\n",
    "except ValueError as e:\n",
    "    print(f'Search error: {e}')\n",
    "except Exception as e:\n",
    "    print(f'Unexpected error: {e}')\n",
    "\n",
    "'''\n",
    "Note: \"Search error: can only read bytes or file-like objects with engine='scipy' or 'h5netcdf'\" seems to indicate a FileNotFoundError,\n",
    "signaling a problem at the data center.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47e333f5-5f1b-4292-9ff6-414635c97127",
   "metadata": {},
   "outputs": [],
   "source": [
    "# View dataset\n",
    "bedmachine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5300263-ab16-41f9-b7ce-1a743a9bf98c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Open local copy of file if streaming fails\n",
    "# bedmachine = xr.open_dataset('/home/jovyan/temp/BedMachineAntarctica-v3.nc')\n",
    "# bedmachine"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "272c7fa5-4ca4-42c1-85f7-3147ac79d2d7",
   "metadata": {},
   "source": [
    "## CryoSat-2 SARIn ATL14 DEM and ATL15 delta h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8db89533-7bcb-4c7e-b744-5db9adf6f635",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Import Smith and Sauthoff, 2025 (CryoSat-2 SARIn Height Change and Reference DEM for Antarctica)\n",
    "# # https://doi.org/10.5281/zenodo.14963551\n",
    "# !zenodo_get -o ~/data 10.5281/zenodo.14963551"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8255f2f-eb7f-4bf2-a4aa-70949e024d44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import CryoSat-2 SARIn delta_h and data_count dataset\n",
    "\n",
    "# Specify the variables to keep\n",
    "keep_vars = ['time', 'y', 'x', 'delta_h', 'data_count', 'polar_stereographic']\n",
    "\n",
    "# Combine quadrants into one data set\n",
    "CS2_SARIn_data = combine_quadrants_from_files(DATA_DIR, \n",
    "    ds_prefixes=[\n",
    "                 'CryoSat2_SARIn_delta_h_A1',\n",
    "                 'CryoSat2_SARIn_delta_h_A2',\n",
    "                 'CryoSat2_SARIn_delta_h_A3',\n",
    "                 'CryoSat2_SARIn_delta_h_A4'\n",
    "                ], \n",
    "    keep_vars=keep_vars)\n",
    "\n",
    "# CRS assignment\n",
    "# try:\n",
    "#     # Try to grab the simple EPSG string first\n",
    "#     crs_info = CS2_SARIn_data['polar_stereographic'].attrs['spatial_ref']\n",
    "# except KeyError:\n",
    "#     # Fallback to WKT if 'spatial_ref' isn't there\n",
    "#     crs_info = CS2_SARIn_data['polar_stereographic'].attrs['crs_wkt']\n",
    "    \n",
    "# # Apply the CRS to the current chunk\n",
    "# CS2_SARIn_data.rio.write_crs(crs_info, inplace=True)\n",
    "\n",
    "# View data set\n",
    "CS2_SARIn_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fea3a329-c9f5-4b67-bafa-394ada076057",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import CryoSat-2 SARIn DEM dataset\n",
    "\n",
    "# Specify the variables to keep\n",
    "keep_vars = ['time', 'y', 'x', 'h']\n",
    "\n",
    "# Combine quadrants into one data set\n",
    "CS2_SARIn_DEM = combine_quadrants_from_files(DATA_DIR, \n",
    "    ds_prefixes=[\n",
    "                 'CryoSat2_SARIn_DEM_A1',\n",
    "                 'CryoSat2_SARIn_DEM_A2',\n",
    "                 'CryoSat2_SARIn_DEM_A3',\n",
    "                 'CryoSat2_SARIn_DEM_A4'\n",
    "                ], \n",
    "    keep_vars=keep_vars)\n",
    "\n",
    "# View data set\n",
    "CS2_SARIn_DEM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52699fdb-c363-4131-9e3d-bebff3b08db9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.close()\n",
    "# CS2_SARIn_DEM['h'].plot()\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d385da26-4bb5-47df-a1ad-7af851044def",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create ICESat-2 absolute heights from ATL14 DEM and ATL15 delta_h data\n",
    "CS2_h = CS2_SARIn_DEM['h'] + CS2_SARIn_data['delta_h']\n",
    "CS2_h = CS2_h.rio.write_crs('EPSG:3031') # FIXME: write from crs of DEM or data?\n",
    "\n",
    "# View newly created dataset\n",
    "CS2_h"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e0ab983-3357-4acc-9bef-c2ce8bbcb265",
   "metadata": {},
   "source": [
    "## ICESat-2 ATL14 DEM and ATL15 delta h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb23ab42-1e70-44d4-9a66-a0612b906d7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find ICESat-2 ATL14 v004 data granules\n",
    "results = earthaccess.search_data(\n",
    "    doi='10.5067/ATLAS/ATL14.004',\n",
    "    bounding_box=(180, -90, -180, -60),  # (lower_left_lon, lower_left_lat , upper_right_lon, upper_right_lat))\n",
    "    cloud_hosted=True,\n",
    ")\n",
    "\n",
    "# Open data granules as s3 files to stream\n",
    "files = earthaccess.open(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0ac7296-e468-441a-8c05-2b6b43763f2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dd372c6-2e98-42fe-9c2a-512022d476fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "variables_to_keep = ['x', 'y', 'Polar_Stereographic', 'h']\n",
    "\n",
    "IS2_ATL14 = combine_quadrants_from_files_S3(\n",
    "    datasets_files=files,\n",
    "    ds_prefixes=[\n",
    "        'ATL14_A1',\n",
    "        'ATL14_A2',\n",
    "        'ATL14_A3',\n",
    "        'ATL14_A4'\n",
    "    ],\n",
    "    keep_vars=variables_to_keep\n",
    ")\n",
    "\n",
    "# View dataset\n",
    "IS2_ATL14"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a6832ee-fbf7-4ad9-8f76-fe3af72895f2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # Authenticate with Earthdata Login\n",
    "# earthaccess.login()\n",
    "\n",
    "# Find ICESat-2 ATL15 v004 data granules\n",
    "results = earthaccess.search_data(\n",
    "    doi='10.5067/ATLAS/ATL15.004',\n",
    "    bounding_box=(180, -90, -180, -60),  # (lower_left_lon, lower_left_lat , upper_right_lon, upper_right_lat))\n",
    "    cloud_hosted=True,\n",
    ")\n",
    "\n",
    "# Open data granules as s3 files to stream\n",
    "files = earthaccess.open(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "480f5d95-9fed-4275-a0ac-88c601d9409c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter to 1 km resolution data sets\n",
    "# filtered_files = [f for f in files if '01km' in str(f)]\n",
    "filtered_files = [f for f in files if '01km' in str(f) and 'ATL15' in str(f)]\n",
    "\n",
    "# Delete intermediary objects for memory conservation\n",
    "del results, files\n",
    "\n",
    "# Sort alphabetically by the data set file name\n",
    "filtered_files.sort(key=lambda x: str(x).split('/')[-1])\n",
    "\n",
    "# Display filtered list\n",
    "filtered_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4893f6cf-b708-42d0-bc6c-2964a1b4f3c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "variables_to_keep = ['x', 'y', 'time', 'Polar_Stereographic', 'delta_h']\n",
    "\n",
    "IS2_ATL15 = combine_quadrants_from_files_S3(\n",
    "    datasets_files=filtered_files,\n",
    "    ds_prefixes=[\n",
    "        'ATL15_A1',\n",
    "        'ATL15_A2',\n",
    "        'ATL15_A3',\n",
    "        'ATL15_A4'\n",
    "    ],\n",
    "    group='delta_h',\n",
    "    keep_vars=variables_to_keep\n",
    ")\n",
    "\n",
    "# View dataset\n",
    "IS2_ATL15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65cb9db8-93d3-4fcd-ac13-b23f2db0f500",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Add datasets attributes\n",
    "# IS2_ATL15.attrs['identifier_product_DOI'] = 'doi:10.5067/ATLAS/ATL15.004'\n",
    "# IS2_ATL15.attrs['shortName'] = 'ATL15'\n",
    "\n",
    "# # View data set\n",
    "# IS2_ATL15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c0d6118-939a-4385-a935-60b8ba2e5e20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create ICESat-2 absolute heights from ATL14 DEM and ATL15 delta_h data\n",
    "IS2_h = IS2_ATL14['h'] + IS2_ATL15['delta_h']\n",
    "\n",
    "# View newly created dataset\n",
    "IS2_h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaf7756c-5c66-4453-b1ed-da95ea73cce9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.close()\n",
    "# IS2_h[:,:,0].plot()\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "271933db-6139-444e-b0d5-64bf8d9734b7",
   "metadata": {},
   "source": [
    "## Pre-process CryoSat-2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56038bf8-e699-4679-87aa-57522f96aaad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clip to SARIn mode mask to eliminate extrapolated data beyond observations\n",
    "CS2_h = CS2_h.chunk({'time': 10})\n",
    "CS2_h_clipped = CS2_h.groupby('time').map(\n",
    "    lambda x: selective_clip(x,\n",
    "        geom_early=gdf_SARIn_3_1.geometry,\n",
    "        geom_late=gdf_SARIn_3_6.geometry)\n",
    ").compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "643ce5f6-eea3-4812-a186-05ec73e5f43c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clip temporally as to not overlap with ICESat-2 data\n",
    "\n",
    "# Remove time slices that occur during the ICESat-2 era that will not be used \n",
    "# to conserve memory when loaded for data analysis\n",
    "\n",
    "# end_date includes one quarter of overlapping data with ICESat-2 time series\n",
    "# to allow for cyc-to-cyc differencing to remove datum from delta_h to create cycle-to-cycle dh\n",
    "# end_date = '2019-01-01T00:00:00.000000000' # Extra time step required if doing cycle-to-cycle dh\n",
    "end_date = '2018-10-01T18:00:00.000000000'\n",
    "\n",
    "CS2_h_clipped = CS2_h_clipped.sel(time=slice(None, end_date))\n",
    "\n",
    "# Preview temporally subset data set's time variable\n",
    "CS2_h_clipped['time']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1589ccb8-42db-4e42-bee3-1f4e3668147c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reindex the smaller one onto the larger grid\n",
    "CS2_on_IS2 = CS2_h_clipped.reindex(y=IS2_h.y, x=IS2_h.x)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41a2aa07-454c-4e33-9a1f-aadb2c3734bb",
   "metadata": {},
   "source": [
    "## Multi-mission absolute height time series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48228929-3829-41f3-9159-ea410de5ac58",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 2) Concatenate along time\n",
    "CS2_IS2_h = xr.concat([CS2_on_IS2, IS2_h], dim=\"time\")\n",
    "\n",
    "# # 3) Optional: sort by time\n",
    "# CS2_IS2_h = CS2_IS2_h.sortby(\"time\")\n",
    "\n",
    "# View dataset\n",
    "CS2_IS2_h['time']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dac978d-2874-415b-9c31-9c172be8636e",
   "metadata": {},
   "outputs": [],
   "source": [
    "CS2_IS2_h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "021499a4-1203-4319-a655-a3b4ad6959b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.close()\n",
    "# CS2_IS2_h[:,:,-1].plot()\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "713ada33-5f16-4840-8acc-8a33d41fd77a",
   "metadata": {},
   "source": [
    "# Data analysis\n",
    "Use Shreve (1972) equation to calculate hydropotential using bed elevations and ice-surface elevations (less firn depth to get ice mass)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "434314d1-3eea-465d-a245-cff1d8361ae6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # Step 1: Compute hydropotential\n",
    "\n",
    "# # Use BedMachine bed topography for Zbed and surface topography (less firn) for Zsurf\n",
    "\n",
    "# # Define densities (rho) of ice and water\n",
    "# rho_ice = 917\n",
    "# rho_water = 997\n",
    "\n",
    "# # Calculate hydropotential using Shreve, 1972 equation\n",
    "# dynamic_subglacial_hydropotential = (9.8 * ((rho_ice*(bedmachine['surface']-bedmachine['firn'])) + (rho_water-rho_ice)*bedmachine['bed'])) / 1e3\n",
    "\n",
    "# # Display xarray.Dataset metadata\n",
    "# dynamic_subglacial_hydropotential"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c69c2bc4-4e6e-4f84-a9fb-e66265935373",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Step 2: Mask to grounded ice only\n",
    "\n",
    "# # Create a mask where values == 2 (grounded ice)\n",
    "# grounded_ice_mask = (bedmachine['mask'] == 2)\n",
    "\n",
    "# # Apply the mask to dynamic_subglacial_hydropotential\n",
    "# dynamic_subglacial_hydropotential = dynamic_subglacial_hydropotential.where(grounded_ice_mask)\n",
    "\n",
    "# # Display xarray.Dataset metadata to ensure edges have become nan's\n",
    "# dynamic_subglacial_hydropotential"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1d5cb81-2ccc-42b2-b2cc-07f4dd5d97d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Step 1: Compute hydropotential\n",
    "\n",
    "# Assign new data variable of hydropotential to ATL15 xarray.Dataset\n",
    "# Follows Shreve 1972, where h_i=Zsurf-Zbed\n",
    "# Adds g (gravitational acceleration) to make units Pa\n",
    "# Use BedMachine Antarctica bed topography for Zbed\n",
    "rho_ice = 917\n",
    "rho_water = 997\n",
    "dynamic_subglacial_hydropotential = (9.8 * (rho_ice*(CS2_IS2_h-bedmachine['firn']) + (rho_water-rho_ice)*bedmachine['bed']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50e67f5e-cd01-4f4f-86fa-f1d51b23b96f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# View newly created datset\n",
    "dynamic_subglacial_hydropotential"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e4cc0ef-c3f1-4a89-bfba-32a6427c4216",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def get_data_extent_with_buffer(da, buffer_percent=0.05):\n",
    "#     \"\"\"\n",
    "#     Find the extent of non-NaN data and add a buffer.\n",
    "    \n",
    "#     Parameters:\n",
    "#     -----------\n",
    "#     da : xarray.DataArray\n",
    "#         The data array to analyze\n",
    "#     buffer_percent : float\n",
    "#         Buffer as a percentage of the data range (default 5%)\n",
    "    \n",
    "#     Returns:\n",
    "#     --------\n",
    "#     tuple : (x_min, x_max, y_min, y_max) with buffer applied\n",
    "#     \"\"\"\n",
    "#     # Find where data is not NaN\n",
    "#     valid_data = ~np.isnan(da)\n",
    "    \n",
    "#     # Get coordinates where we have valid data\n",
    "#     y_coords, x_coords = np.where(valid_data)\n",
    "    \n",
    "#     if len(x_coords) == 0 or len(y_coords) == 0:\n",
    "#         print(\"Warning: No valid data found!\")\n",
    "#         return None\n",
    "    \n",
    "#     # Convert array indices to actual coordinate values\n",
    "#     x_values = da.coords[da.dims[1]].values  # assuming dims are [y, x]\n",
    "#     y_values = da.coords[da.dims[0]].values\n",
    "\n",
    "#     # Get the actual coordinate bounds where we have data\n",
    "#     x_data_min = x_values.min()\n",
    "#     x_data_max = x_values.max()\n",
    "#     y_data_min = y_values.min()\n",
    "#     y_data_max = y_values.max()\n",
    "    \n",
    "#     # Calculate buffer\n",
    "#     x_range = x_data_max - x_data_min\n",
    "#     y_range = y_data_max - y_data_min\n",
    "    \n",
    "#     x_buffer = x_range * buffer_percent\n",
    "#     y_buffer = y_range * buffer_percent\n",
    "    \n",
    "#     # Apply buffer\n",
    "#     x_min = x_data_min - x_buffer\n",
    "#     x_max = x_data_max + x_buffer\n",
    "#     y_min = y_data_min - y_buffer\n",
    "#     y_max = y_data_max + y_buffer\n",
    "    \n",
    "#     return x_min, x_max, y_min, y_max\n",
    "\n",
    "# # Get the data extent with 5% buffer\n",
    "# extent = get_data_extent_with_buffer(dynamic_subglacial_hydropotential[:,:,0], buffer_percent=0.01)\n",
    "\n",
    "# if extent is not None:\n",
    "#     x_min, x_max, y_min, y_max = extent\n",
    "    \n",
    "#     # Plot results and save figure\n",
    "#     fig, ax = plt.subplots(figsize=(10, 8))\n",
    "\n",
    "#     # Use actual data range for asymmetric colorbar\n",
    "#     data_min = float(dynamic_subglacial_hydropotential.min())\n",
    "#     data_max = float(dynamic_subglacial_hydropotential.max())\n",
    "\n",
    "#     # Create the plot\n",
    "#     im = dynamic_subglacial_hydropotential[:,:,-1].plot(\n",
    "#         ax=ax, \n",
    "#         add_colorbar=False,\n",
    "#         cmap='viridis',\n",
    "#         vmin=data_min,\n",
    "#         vmax=data_max\n",
    "#     )\n",
    "\n",
    "#     # Set the axis limits to clip to data extent + buffer\n",
    "#     ax.set_xlim(x_min, x_max)\n",
    "#     ax.set_ylim(y_min, y_max)\n",
    "    \n",
    "#     # Format axes\n",
    "#     ax.set_aspect('equal')\n",
    "#     ax.set_axis_off()\n",
    "\n",
    "#     # Create colorbar axes with same width as plot\n",
    "#     divider = make_axes_locatable(ax)\n",
    "#     cax = divider.append_axes('bottom', size='3%', pad=0.01)\n",
    "    \n",
    "#     # Add colorbar with label\n",
    "#     cbar = plt.colorbar(im, cax=cax, orientation='horizontal')\n",
    "#     cbar.set_label('subglacial hydropotential [kPa]')\n",
    "#     cbar.ax.tick_params(labelrotation=45)\n",
    "    \n",
    "#     plt.tight_layout()\n",
    "#     plt.show()\n",
    "\n",
    "#     plt.savefig('output/subglacial_hydropotential_Antarctica.png', bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0ad5068-1427-4312-a9d6-ce1cbef5e972",
   "metadata": {},
   "source": [
    "# Export hydropotental dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d9634b4-14c8-4682-808b-2cb64fdf7bed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare dataset for export\n",
    "\n",
    "# Add variable metadata\n",
    "dynamic_subglacial_hydropotential.attrs = {\n",
    "    'units': 'Pa',\n",
    "    'ice_density' : 917.0,\n",
    "    'ice_density_units': 'kg m-3',\n",
    "    'freshwater_density' : 997.0,\n",
    "    'freshwater_density_units': 'kg m-3',\n",
    "}\n",
    "\n",
    "# Convert to dataset for chucking with Zarr\n",
    "ds = dynamic_subglacial_hydropotential.to_dataset(name='dynamic_subglacial_hydropotential')\n",
    "\n",
    "# Add global metadata\n",
    "ds.attrs = {\n",
    "    'conventions': 'CF-1.8',\n",
    "    'title': 'Dynamic Antarctic subglacial hydropotential',\n",
    "    'description': 'Dynamic Antarctic subglacial gridded hydropotential calculated using Shreve (1972) equation \\\n",
    "        using BedMachine Antarctica v3 bed topography and firn air content (Morlighem et al., 2020; Morlighem, 2022) \\\n",
    "        and ice surface heights from CryoSat-2 SARIn mode (Smith & Sauthoff, 2025) and ICESat-2 ATL14/15 (Smith and others, 2024a,b).',\n",
    "    'history': 'Created 2025-07-20',\n",
    "    'identifier_product_DOI': 'doi:10.5281/zenodo.16243278',\n",
    "    'citation': 'Sauthoff, W. & Siegfried, M. R. (2025). Antarctic subglacial hydropotential [Data set]. Zenodo. https://doi.org/10.5281/zenodo.16243278',\n",
    "    'license': 'CC BY-SA 4.0',\n",
    "    'region': 'Antarctica'\n",
    "}\n",
    "\n",
    "# Copy mapping variable into new dataset as variable with zero dimensions\n",
    "mapping_attrs = bedmachine['mapping'].attrs\n",
    "ds['mapping'] = xr.DataArray(np.array(0, dtype=np.int32), attrs=mapping_attrs)\n",
    "\n",
    "# Link the mapping variable to hydropotential variable\n",
    "ds['dynamic_subglacial_hydropotential'].attrs['grid_mapping'] = 'mapping'\n",
    "\n",
    "# Copy global CRS-related attributes\n",
    "ds.attrs['proj4'] = bedmachine.attrs['proj4']\n",
    "ds.attrs['Projection'] = bedmachine.attrs['Projection']\n",
    "\n",
    "# Add crs and crs_wkt\n",
    "ds.attrs['crs'] = 'EPSG:3031'\n",
    "crs = CRS.from_epsg(3031)\n",
    "ds.attrs['crs_wkt'] = crs.to_wkt()\n",
    "\n",
    "# View ds metadata\n",
    "ds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2a15be8-8272-4871-a4e9-28c1af97adbc",
   "metadata": {},
   "source": [
    "## Export to chucked netcdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "191daac3-3c87-478b-9167-5190b2d6bfc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export with chunking + compression\n",
    "ds.to_netcdf(\n",
    "    'output/dynamic_subglacial_hydropotential_Antarctica.nc',\n",
    "    encoding={\n",
    "        'dynamic_subglacial_hydropotential': {\n",
    "            'dtype': 'float32',\n",
    "            'zlib': True,\n",
    "            'complevel': 4,\n",
    "            'chunksizes': (500, 500, 1)  # (y, x, time)\n",
    "        }\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7af32a5-6175-4a7f-b6b2-d3f067ab05e4",
   "metadata": {},
   "source": [
    "## Validate dataset after export\n",
    "First check climate and forecast conventions, then importing exported file, viewing metadata and plotting data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80a6dfbe-b1ee-4a1d-9411-c47bd8657ec6",
   "metadata": {},
   "outputs": [],
   "source": [
    "!conda install -c conda-forge cfchecker --quiet -y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1e867c7-0847-4d89-88fc-5670a80cf2fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "!cfchecks output/dynamic_subglacial_hydropotential_Antarctica.nc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40b51100-9be0-4e3a-b3ee-92bc31c980b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_imported = xr.open_dataset('output/dynamic_subglacial_hydropotential_Antarctica.nc')\n",
    "ds_imported"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90faed20-a6ad-4fab-8a24-093250cd896a",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_imported.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "809541ab-d8d9-4a47-9508-6ea0715a84c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check that NaN values are preserved\n",
    "print('Original NaN count:', ds['dynamic_subglacial_hydropotential'].isnull().sum())\n",
    "print('Imported NaN count:', ds_imported['dynamic_subglacial_hydropotential'].isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "436f2baa-f999-464b-ad31-5ef9160d2e6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "ds_imported['dynamic_subglacial_hydropotential'][:,:,-1].plot()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c93f4770-57db-4dc4-96f2-2dac518e3791",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Opened netcdf in Panoply to ensure compatibility"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40c80c6a-2e4d-4293-a670-36171c21d07b",
   "metadata": {},
   "source": [
    "## Export to Zarr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6ad75e4-fbc1-41db-83bf-8fc2dc72129d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chunk for performance (tune based on size and use case)\n",
    "ds = ds.chunk({'x': 500, 'y': 500})\n",
    "\n",
    "# Write to a Zarr store\n",
    "ds.to_zarr('output/dynamic_subglacial_hydropotential_Antarctica.zarr', mode='w',  consolidated=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a237e47-ec52-441a-a9b4-8b577db4eba9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Zip zarr files for upload to Zenodo\n",
    "shutil.make_archive(\n",
    "    'output/dynamic_subglacial_hydropotential_Antarctica.zarr',  # output path (no zip extension)\n",
    "    'zip',\n",
    "    'output/dynamic_subglacial_hydropotential_Antarctica.zarr'   # source\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e549abde-4bb6-4824-a614-14d6ec763e10",
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm -r 'output/dynamic_subglacial_hydropotential_Antarctica.zarr'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03895bd4-d4f8-4642-ad0a-e0bb7c19a91c",
   "metadata": {},
   "source": [
    "## Validate dataset after export\n",
    "Unzip zip file of Zarr store, then import data, view metadata, plot data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b70fdc4-c8e2-4a8e-8a55-963f3cd1d922",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the zip file\n",
    "with zipfile.ZipFile('output/dynamic_subglacial_hydropotential_Antarctica.zarr.zip', 'r') as zip_ref:\n",
    "    zip_ref.extractall('output/dynamic_subglacial_hydropotential_Antarctica.zarr')\n",
    "\n",
    "# Now open normally\n",
    "ds_imported = xr.open_zarr('output/dynamic_subglacial_hydropotential_Antarctica.zarr', consolidated=True)\n",
    "ds_imported"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63474497-7fda-4366-ab56-b12102e44d8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_imported.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97d2db4b-abe4-4c25-9398-0a41c9542518",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check that NaN values are preserved\n",
    "print('Original NaN count:', ds['subglacial_hydropotential'].isnull().sum().compute())\n",
    "print('Imported NaN count:', ds_imported['subglacial_hydropotential'].isnull().sum().compute())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d97be8fc-5838-424c-8ba1-480d9a845b2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "ds_imported['subglacial_hydropotential'].plot()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d035f75c-bc46-4186-991b-2f685bb243b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove Zarr store folder\n",
    "!rm -r 'output/dynamic_subglacial_hydropotential_Antarctica.zarr'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e14e8586-daf8-49bb-8050-c1f0fa04759b",
   "metadata": {},
   "source": [
    "# Remove temporary files\n",
    "Files are first downloaded locally to upload to Zenodo repo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "268716a7-84d7-44b3-8056-d8a595164852",
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm 'output/dynamic_subglacial_hydropotential_Antarctica.nc'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14cee127-1959-4c6d-9f74-3d467640727c",
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm 'output/dynamic_subglacial_hydropotential_Antarctica.zarr.zip'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6f739ee-72dd-4657-b653-8fd63144264b",
   "metadata": {},
   "source": [
    "# References\n",
    "\n",
    "Morlighem, M., Rignot, E., Binder, T., Blankenship, D., Drews, R., Eagles, G., et al. (2020). Deep glacial troughs and stabilizing ridges unveiled beneath the margins of the Antarctic ice sheet. _Nature Geoscience_, 13(2), 132–137. [doi:10.1038/s41561-019-0510-8](https://doi.org/10.1038/s41561-019-0510-8)\n",
    "\n",
    "Morlighem, M. (2022). MEaSUREs BedMachine Antarctica. (NSIDC-0756, Version 3). [Data Set]. Boulder, Colorado USA. NASA National Snow and Ice Data Center Distributed Active Archive Center. https://doi.org/10.5067/FPSU0V1MWUB6. Date Accessed 05-19-2025.\n",
    "\n",
    "Sauthoff, W., & Siegfried, M. R. (2025). Antarctic subglacial hydropotential [Data set]. Zenodo. [https://doi.org/10.5281/zenodo.16243278](10.5281/zenodo.16243278)\n",
    "\n",
    "Smith, B., & Sauthoff, W. (2025). CryoSat-2 SARIn Height Change and Reference DEM for Antarctica (1.0) [Data set]. Zenodo. [https://doi.org/10.5281/zenodo.14963551](https://doi.org/10.5281/zenodo.14963551)\n",
    "\n",
    "Smith, B., Sutterley, T., Dickinson, S., Jelley, B. P., Felikson, D., Neumann, T. A., Fricker, H. A., Gardner, A. S., Padman, L., Markus, T., Kurtz, N., Bhardwaj, S., Hancock, D. & Lee, J. (2024). ATLAS/ICESat-2 L3B Gridded Antarctic and Arctic Land Ice Height. (ATL14, Version 4). [Data Set]. Boulder, Colorado USA. NASA National Snow and Ice Data Center Distributed Active Archive Center. https://doi.org/10.5067/ATLAS/ATL14.004. Date Accessed 12-01-2025.\n",
    "\n",
    "Smith, B., Sutterley, T., Dickinson, S., Jelley, B. P., Felikson, D., Neumann, T. A., Fricker, H. A., Gardner, A. S., Padman, L., Markus, T., Kurtz, N., Bhardwaj, S., Hancock, D. & Lee, J. (2024). ATLAS/ICESat-2 L3B Gridded Antarctic and Arctic Land Ice Height Change. (ATL15, Version 4). [Data Set]. Boulder, Colorado USA. NASA National Snow and Ice Data Center Distributed Active Archive Center. https://doi.org/10.5067/ATLAS/ATL15.004. Date Accessed 12-01-2025.\n",
    "\n",
    "Shreve, R. L. (1972). Movement of Water in Glaciers. _Journal of Glaciology_, 11(62), 205–214. [doi:10.3189/S002214300002219X](https://doi.org/10.3189/S002214300002219X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb2dc849-ae06-4a4e-a7a0-2b1fdbfd707d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
